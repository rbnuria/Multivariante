%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Plantilla básica de Latex en Español.
%
% Autor: Andrés Herrera Poyatos (https://github.com/andreshp) 
%
% Es una plantilla básica para redactar documentos. Utiliza el paquete fancyhdr para darle un
% estilo moderno pero serio.
%
% La plantilla se encuentra adaptada al español.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-----------------------------------------------------------------------------------------------------
%	INCLUSIÓN DE PAQUETES BÁSICOS
%-----------------------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{lipsum}                     % Texto dummy. Quitar en el documento final.

%-----------------------------------------------------------------------------------------------------
%	SELECCIÓN DEL LENGUAJE
%-----------------------------------------------------------------------------------------------------

% Paquetes para adaptar Látex al Español:
\usepackage[spanish,es-noquoting, es-tabla, es-lcroman]{babel} % Cambia 
\usepackage[utf8]{inputenc}                                    % Permite los acentos.
\selectlanguage{spanish}                                       % Selecciono como lenguaje el Español.

%-----------------------------------------------------------------------------------------------------
%	SELECCIÓN DE LA FUENTE
%-----------------------------------------------------------------------------------------------------

% Fuente utilizada.
\usepackage{courier}                    % Fuente Courier.
\usepackage{microtype}                  % Mejora la letra final de cara al lector.

%-----------------------------------------------------------------------------------------------------
%	ESTILO DE PÁGINA
%-----------------------------------------------------------------------------------------------------

% Paquetes para el diseño de página:
\usepackage{fancyhdr}               % Utilizado para hacer títulos propios.
\usepackage{lastpage}               % Referencia a la última página. Utilizado para el pie de página.
\usepackage{extramarks}             % Marcas extras. Utilizado en pie de página y cabecera.
\usepackage[parfill]{parskip}       % Crea una nueva línea entre párrafos.
\usepackage{geometry}               % Asigna la "geometría" de las páginas.

\usepackage{enumerate} 				% Para cambiar formato enumeración

% Se elige el estilo fancy y márgenes de 3 centímetros.
\pagestyle{fancy}
\geometry{left=3cm,right=3cm,top=3cm,bottom=3cm,headheight=1cm,headsep=0.5cm} % Márgenes y cabecera.
% Se limpia la cabecera y el pie de página para poder rehacerlos luego.
\fancyhf{}

% Espacios en el documento:
\linespread{1.1}                        % Espacio entre líneas.
\setlength\parindent{0pt}               % Selecciona la indentación para cada inicio de párrafo.

% Cabecera del documento. Se ajusta la línea de la cabecera.
\renewcommand\headrule{
	\begin{minipage}{1\textwidth}
		\hrule width \hsize 
	\end{minipage}
}

% Texto de la cabecera:
\lhead{}                          % Parte izquierda.
\chead{}                                    % Centro.
\rhead{\subject \ - \doctitle}              % Parte derecha.

% Pie de página del documento. Se ajusta la línea del pie de página.
\renewcommand\footrule{                                 
	\begin{minipage}{1\textwidth}
		\hrule width \hsize   
	\end{minipage}\par
}

\lfoot{}                                                 % Parte izquierda.
\cfoot{}                                                 % Centro.
\rfoot{Página\ \thepage\ de\ \protect\pageref{LastPage}} % Parte derecha.

%----------------------------------------------------------------------------------------
%	MATEMÁTICAS
%----------------------------------------------------------------------------------------

% Paquetes para matemáticas:                     
\usepackage{amsmath, amsthm, amssymb, amsfonts, amscd} % Teoremas, fuentes y símbolos.

% Nuevo estilo para definiciones
\newtheoremstyle{definition-style} % Nombre del estilo
{8pt}                % Espacio por encima
{6pt}                % Espacio por debajo
{}                   % Fuente del cuerpo
{}                   % Identación: vacío= sin identación, \parindent = identación del parráfo
{\bf}                % Fuente para la cabecera
{.}                  % Puntuación tras la cabecera
{.5em}               % Espacio tras la cabecera: { } = espacio usal entre palabras, \newline = nueva línea
{}                   % Especificación de la cabecera (si se deja vaía implica 'normal')

% Nuevo estilo para teoremas
\newtheoremstyle{theorem-style} % Nombre del estilo
{6pt}                % Espacio por encima
{2pt}                % Espacio por debajo
{\itshape}           % Fuente del cuerpo
{}                   % Identación: vacío= sin identación, \parindent = identación del parráfo
{\bf}                % Fuente para la cabecera
{.}                  % Puntuación tras la cabecera
{.5em}               % Espacio tras la cabecera: { } = espacio usal entre palabras, \newline = nueva línea
{}                   % Especificación de la cabecera (si se deja vaía implica 'normal')

% Nuevo estilo para ejemplos y ejercicios
\newtheoremstyle{example-style} % Nombre del estilo
{9pt}                % Espacio por encima
{2pt}                % Espacio por debajo
{}                   % Fuente del cuerpo
{}                   % Identación: vacío= sin identación, \parindent = identación del parráfo
{\scshape}                % Fuente para la cabecera
{:}                  % Puntuación tras la cabecera
{.5em}               % Espacio tras la cabecera: { } = espacio usal entre palabras, \newline = nueva línea
{}                   % Especificación de la cabecera (si se deja vaía implica 'normal')

% Teoremas:
\theoremstyle{theorem-style}  % Otras posibilidades: plain (por defecto), definition, remark
\newtheorem{theorem}{Teorema}[section]  % [section] indica que el contador se reinicia cada sección
\newtheorem{corollary}[theorem]{Corolario} % [theorem] indica que comparte el contador con theorem
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{proposition}[theorem]{Proposición}

% Definiciones, notas, conjeturas
\theoremstyle{definition}
\newtheorem{definition}{Definición}[section]
\newtheorem{conjecture}{Conjetura}[section]
\newtheorem*{note}{Nota} % * indica que no tiene contador

% Ejemplos, ejercicios
\theoremstyle{example-style}
\newtheorem{example}{Ejemplo}[section]
\newtheorem{exercise}{Ejercicio}[section]

%-----------------------------------------------------------------------------------------------------
%	PORTADA
%-----------------------------------------------------------------------------------------------------

% Elija uno de los siguientes formatos.
% No olvide incluir los archivos .sty asociados en el directorio del documento.
\usepackage{title1}
%\usepackage{title2}
%\usepackage{title3}

%-----------------------------------------------------------------------------------------------------
%	TÍTULO, AUTOR Y OTROS DATOS DEL DOCUMENTO
%-----------------------------------------------------------------------------------------------------

% Título del documento.
\newcommand{\doctitle}{Complementos del álgebra matricial. Derivación matricial.}
% Subtítulo.
\newcommand{\docsubtitle}{Trabajo B}
% Fecha.
\newcommand{\docdate}{10 \ de \ enero \ de \ 2018}
% Asignatura.
\newcommand{\subject}{Estadística Multivariante}
% Autor.
\newcommand{\docauthor}{Antonio R. Moya Martín-Castaño \\Elena Romero Contreras \\Nuria Rodríguez Barroso}
\newcommand{\docaddress}{Universidad de Granada}
\newcommand{\docemail}{anmomar85@correo.ugr.es \\ elenaromeroc@correo.ugr.es \\ rbnuria6@gmail.com}

%-----------------------------------------------------------------------------------------------------
%	RESUMEN
%-------------------------------					----------------------------------------------------------------------

% Resumen del documento. Va en la portada.
% Puedes también dejarlo vacío, en cuyo caso no aparece en la portada.
%\newcommand{\docabstract}{}
\newcommand{\docabstract}{}

\begin{document}

 \maketitle

%-----------------------------------------------------------------------------------------------------
%	ÍNDICE
%-----------------------------------------------------------------------------------------------------

% Profundidad del Índice:
%\setcounter{tocdepth}{1}

\newpage
\tableofcontents
\newpage

\section{Complementos del álgebra matricial.}

\subsection{Introducción}

A pesar de que la manipulación algebraica de vectores aleatorios puede ser abordada sin ninguna complicación, las matrices aleatorias suponen un esfuerzo extra ya que el resultado obtenido sobre las mismas debe generalizar al realizado sobre vectores. Además, el uso de matrices aleatorias en el Análisis Multivariante es muy importante dado que aparecen de forma natural, por ejemplo, al estimar la matriz de covarianzas de una población vectorial.

\subsection{Operación Vec.}

Para que el trato de las matrices aleatorias generalice a la manipulación de vectores aleatorios, debemos \textit{vectorizar} las matrices, esto es, tratarlas como si se tratasen de vectores. Esta consideración se puede hacer teniend oen cuenta que los espacios $\mathbb{M_{n \times q}}$ y $\mathbb{R^{nq}}$ son isomorfos. 

Vemos que el objetivo puede ser muy cómodo a la hora de manipular matrices aleatorias, sin embargo, debemos entender primero bien las propiedades que unen las expresiones matriciales y las \textit{vectorizadas}. Introducimos formalmente la definición de \textit{Vec}

\vspace{0.2cm}

\begin{definition}
\textit{	Sea \textbf{X} una matriz de orden $n \times q$. Se define Vec(\textbf{X}) como el vector de dimensión $nq \times 1$ formado al apilar las columnas de X una tas otra, o sea, si notamos por columnas \textit{X} = $[x_1, x_2, ..., x_1]$, entonces}

$$ Vec(X) = \left({\begin{array}{c}
	x_1\\
	x_2\\
	.\\
	.\\
	.\\
	\\
	x_q
	\end{array} } \right)$$
\end{definition}

Como ya habíamos comentado, la identificación entre matrices y las \textit{vectorizadas} se basa en un resultado que formalizamos a continuación:

\begin{theorem}
	\textit{La aplicación Vec: $\mathbb{M}_{n \times q} \rightarrow \mathbb{R}^{n1}$ es un isomorfismo de espacios vectoriales.}
\end{theorem}


Este isomorfismo presentado en este teorema nos puede servir para calcular la esperanza matemática. 
Así, cuando sea más fácil calcular al esperanza de su vectorización, utilizando la propiedad que obtenemos el en ejemplo siguiente, bastaría con deshacer el cambio con el isomorfismo anterior.


\begin{example}
	\textit{Sean $x_1, ... , x_N$  vectores aleatorios p-dimensionales con igual media $\mu$. Sea la matriz aleatoria \textbf{X}$_{N \times X } = [x_1, ..., x_N]^t$ y consideremos el vector}
	
	$$ Vec(X^t) = \left({\begin{array}{c}
		x_1\\
		x_2\\
		.\\
		.\\
		.\\
		\\
		x_N
		\end{array} } \right)$$
	
	\textit{Entonces, si notamos \textbf{1}$_N$ al vector N dimensional cuyas componentes son todas iguales a uno, se verifica}
	
	$$ E[Vec(X^t)] = \left({\begin{array}{c}
		\mu\\
		\mu\\
		.\\
		.\\
		.\\
		\\
		\mu
		\end{array} } \right) = Vec([\mu, \mu, ..., \mu]) = Vec(\mu 1_N^t)$$
\end{example}


\subsection{Producto Kronecker}

	Como ya hemos visto en el ejercicio anterior, utilizar la vectorización puede facilitar ciertos cálculos, como el de la esperanza de una matriz aleatoria. Sin embargo, hay ocasiones en las que es obligatorio usar la vectorización de matrices, como en el ejemplo siguiente:
	
	\begin{example}
		\textit{En las hipótesis del ejercicio anterior, suponemos además que $x_1, ..., x_N$ son independientes y con igual matriz de covarianzas $\Sigma$. Entonces se define la matriz de covarianzas de \textbf{X}$^t$ como $Cov[Vec(X^t)]$ ya que dicha matriz contiene todas las matrices de covarianzas entre las columnas de \textbf{X}$^t$. A partir de la definición de la matriz de covarianzas de un vector aleatorio, se verifica}
		
		****** (poner definición del apéndice B)
		
		$$Cov[Vec(X^t)] = E[[Vec(X^t)  - E[Vec(X^t)]] [Vec(X^t) - E[Vec(X^t)]]^t] = $$
		
		
		$$=E \left[{\begin{array}{c}
			X_1 - \mu\\
			X_2 - \mu\\
			.\\
			.\\
			.\\
			X_N - \mu\\
			\mu
			\end{array}}{\begin{array}{ccc}
			(X_1 - \mu)^t & ... &	(X_N- \mu)^t\\
			\end{array} }\right] =	 \left( {\begin{array}{cccc}
			\Sigma & 0 & ... & 0\\
			0 & \Sigma & ... & 0\\
			... & ... & .. & ... \\
			0 & ... & 0 & \Sigma\mu
			\end{array}}\right) $$
		
	\end{example}

	A raíz de la expresión obtenida en el ejemplo, vamos a definir el producto de Kronecker de matrices:
	
	\begin{definition}
		\textit{Sean $A_{m \times n}$ y $B_{p \times q}$ dos matrices. Se define el producto Kronecker de ellas como la matriz de dimensiones $mp \times nq$ siguiente}
		
		$$A \otimes B =  \left( {\begin{array}{cccc}
			a_{11}B & a_{12}B & ... & a_{1n}B\\
			a_{21}B & a_{22}B & ... & a_{2n}B\\
			... & ... & .. & ... \\
			a_{m1}B & a_{m2}B  & ... & a_{mn}B
			\end{array}}\right) = (a_{ij}B)_{ij} ; i = 1,.., m, j = 1,.., n$$
		
	\end{definition}
	
	Observamos la utilidad de la definición anterior obteniendo que en el ejemplo anterior se tiene $Cov[Vec(X^t)] = I_N \otimes \Sigma$.
	
	Aunque hemos utilizado el resultado anterior para justificar la introducción del producto Kronecker, no es una justificación formal. Otra de las justificaciones que podríamos haber dado para introducirlo podría ser  el resolver el sistema de ecuaciones $x = (A \otimes B)y$, qeu es no singular. 
	
	Ahora bien, una vez presentada esta nueva operación nos proponemos estudiar sus propiedades. Al igual que el producto usual matricial estaba relacionado con la composición de aplicaciones lineales, este nuevo producto lo estará con el producto tensorial. Vemos algunas propiedades en el siguiente teorema.
	
	\begin{theorem}
		\textit{Se verifican las siguientes propiedades:}
		
		\begin{enumerate}
			\item \textit{Dados $\alpha, \beta \in \mathbb{R}$,  $A_{m \times n}$ y $B_{p \times q}$, entonces}
			
			$$(\alpha A) \otimes (\beta B) = \alpha \beta (A \otimes B) = \alpha \beta A \otimes B = A \otimes (\alpha \beta )B$$
			
			\item \textit{Dadas $A_{m \times n}, B_{m \times n}, C_{p \times q}$ y $D_{p \times q}$, entonces}
				\begin{enumerate}
					\item $(A \otimes C)+(B \otimes C) = (A+B) \otimes C$
					\item $(A \otimes C) + (A \otimes D) = A \otimes (C+D)$
					\item $(A + B) \otimes (C+D) = (A \otimes C) + (A \otimes D) + (B \otimes C) + (B \otimes D)$
				\end{enumerate}
			
			\item \textit{Dadas $A_{m \times n}, B_{p \times q}, C_{r \times s}, entonces (A \otimes B) \otimes C = A \otimes (B \otimes C)$}
			
			\item \textit{Dadas $A_{m \times n}, B_{n \times p}, C_{q \times r}$ y $D_{r \times s}$, entonces $(A \otimes C)(B \otimes D) = AB \otimes CD$}
			
			\item \textit{Dadas $A_{m \times m}$ y $B_{n \times n}$ son no singualres entonces $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$}
			
			\item \textit{Si $A_{m \times n}$ y $B_{p \times q}$, entonces $(A \otimes B)^t = A^t \otimes B^t$}
			
			\item \textit{Si $A_{m \times m}$ y $B_{n \times n}$ son ortogonales, entonces $A \times B$ es ortogonal}
			
			\item \textit{Si $A_{m \times m} $y $B_{n \times n}$ son matrices triangulares (inferiores), entonces $A \otimes B$ es triangular superior (inferior).}
			
			\item \textit{Si $A_{m \times m}$ y $B_{n \times n}$ son definidas positivas, entonces $A \otimes B$ es definida positiva}
			
			\item \textit{Dadas $A_{m \times n} = [A_1, ..., A_k]$ y $B_{p \times q}$, entonces $A \otimes B ? [A_1 \otimes B, ..., A_k \otimes B]. En particular, si a_{m \times 1}$ y $b_{p \times 1}$ son dos vectores se tiene que $a \otimes b^t = ab^t = b^t \otimes a$ }
			
			\item \textit{Dadas $A_{m \times n} =\left( {\begin{array}{cc}
						A_{11} & A_{12}\\
						A_{21} & A_{22}\\
				\end{array}}\right)$ 			 y $B_{p \times q}$ , entonces $A \otimes B = \left( {\begin{array}{cc}
					A_{11} \otimes B & A_{12} \otimes B\\
					A_{21}  \otimes B & A_{22} \otimes B
					\end{array}}\right)  $}
			
			\item \textit{Dadas $A_{m \times m}$ y $B_{n \times n}$, entonces $tr[A \otimes B$] = $tr[A]tr[B]$}
			
			\item \textit{Sean $A_{m \times m}$ y $B_{n \times n}$ matrices reales con autovalores realies respectivos $\lambda_1, ..., \lambda_2$ y $\mu_1, ..., \mu_n$. Entonces $A \otimes B$ tiene como autovalores $\lambda_i \mu_j, i = 1, ..., m; j = 1,...,n$. Como consecuencia $rg(A \otimes B) = rg(A)rg(B)$}
			
			\item \textit{Dadas $A_{m \times m }$ y $B_{n \times n}$, entonces $|A \otimes B| = |A|^n |B|^m$}
			
			
		\end{enumerate}
	\end{theorem}
	
	
\subsubsection{Producto Kronecer y Vec. Relaciones de interés entre ambas operaciones.}
	

\newpage
\section{Derivación matricial.}

\subsection{Introducción}

En este apartado se va a tratar la derivación respecto a vectores y matrices, que es muy necesaria en estadística multivariante sobre todo desde el punto de vista de la optimización. Así, permite calcular datos tales como estimador máximo verosímil, matrices de información de Fisher, o cotas tipo Crámer-Rao. Más importancia tiene todavía este tema si tenemos en cuenta que, si ya la derivación vectorial puede dar lugar a cálculos costosos, en el caso de la matricial se pueden generar un enorme número de derivadas que pueden resultar difícil de ordenar con sentido en una matriz. \\

Muchas son las aproximaciones que se han dado y en este caso lo que se hará es seguir una línea en la que las matrices se conviertan en vectores, mucho más fáciles de manejar. **********

\subsection{Diferencial primera y jacobianos}

\subsubsection{Diferencial de una función vectorial}
\textbf{Definición 2.2.1}: Consideramos una función vectorial $f: S \rightarrow \mathbb{R}^m$ con $S\subset \mathbb{R}^n$. Sea \textbf{c} un punto interior de S y consideremos una bola cerrada con centro en \textbf{c} y radio \textbf{r}, $B(c,r)$. Sea $u$ un punto de $\mathbb{R}^n$ tal que $||u||\leq r$ es decir, $c+r \in B(c,r)$. \\
Diremos que f es \textbf{diferenciable} en \textbf{c} si existe una matriz real de orden $m$ x $n$ que depende de $c$ y no de $u$ y que cumple que $f(c+u)-f(c) = A(c)u + r_c(u)$ con $\lim_{u\to0} \frac{r_c(u)}{||u||} = 0$. Además, se define la \textbf{primera diferencial} de $f$ en el punto $c$ con incremento $u$ como: $df(c;u)=A(c)u$.\\

\textbf{Definición 2.2.2}: Sea $f: S\subset \mathbb{R}^n \rightarrow \mathbb{R}^m$ y sea $f_i: S \rightarrow \mathbb{R}$ su i-ésima componente. Sea $c$ un punto interior de S y $e_j$ el j-ésimo vector de la base canónica de $\mathbb{R}^n$. Se define la \textbf{derivada parcial} de $f$ respecto a la j-ésima coordenada como: 

$$ D_jf_i(c) = \lim_{t\to0} \frac{f_i(c + te_j ) - f_i(c)}{t}, t\in \mathbb{R} $$  
 
\begin{theorem} (Primer teorema de identificación para funciones vectoriales)\\
	Sea $f: S\subset \mathbb{R}^n \rightarrow \mathbb{R}^m$ diferenciable en un punto $c$ interior de S y $u  \in \mathbb{R}^n$. Entonces $df(c;u)=(Df(c))u$ donde $Df(c)$ es una matriz $m$x$n$ cuyos elementos $D_jf_i(c)$ son las derivadas parciales de \textbf{f} evaluadas en $c$ y que recibe el nombre de matriz jacobiana. Recíprocamente, si $A(c)$ es una matriz que verifica que $df(c;u)=A(c)u \forall u\in \mathbb{R}^n$, entonces $A(c) = Df(c)$. 
\end{theorem}

El siguiente teorema nos proporciona la regla de la cadena para funciones vectoriales.

\begin{theorem}
	Sea $f: S\subset \mathbb{R}^n \rightarrow \mathbb{R}^m$ diferenciable en un punto $c$ interior de S. Sea $T$ un subconjunto de $\mathbb{R}^m$ tal que $f(x) \in T \forall x \in S$ y supongamos que $g: T \rightarrow \mathbb{R}^p$ es diferenciable en un punto \textbf{b} (b = f(c)) de T. Entonces la función compuesta $h: S \rightarrow \mathbb{R}^p$ definida por $h(x)=g(f(x))$, es diferenciable en $c$ y $Dh(c)=Dg(b)Df(c)$.
\end{theorem}

\begin{theorem} (Regla de invarianza de Cauchy) \\
	En el ambiente del teorema anterior, si \textbf{f} es diferenciable en \textbf{c} y \textbf{g} lo es en \textbf{b=f(c)}, entonces la diferenciable $h=g$o$f$ es $dh(c;u) = dg(b;df(c;u))$.   
	
\end{theorem}

\subsubsection{Diferencial de una función matricial}

Basta con extrapolar lo dicho en el caso vectorial usando la operación Vec.

Consideramos una función matricial $F: S \rightarrow \mathbb{M}_{m\times p}$ donde $S \subseteq \mathbb{M}_{n\times q}$. Sea C un punto itnerior de $S$, $\mathbb{B}(C;r) \subseteq S$ una bola abierta y $U$ un punto de $\mathbb{M}_{n\times q}$ con $||U||<r$, por lo que $C+U$ pertenece a $\mathbb{B}(C;r)$, donde hemos considerado la norma matricial $||U||=(\text{tr}[U^tU])^{\frac{1}{2}}$.

\begin{definition}
\textit{	En las condiciones anteriores, se dice que $F$ es diferenciable en $C$ si existe una matriz $A$ de dimensiones $mp\times nq$, que dependa de $C$ y no de $U$ y tal que}
	$$\text{Vec}(F(C+U))-\text{Vec}(F(C))=A(C)\text{Vec}(U)+\text{Vec}(R_C(U))\; \text{ donde } \lim\limits_{U\rightarrow 0} \frac{R_C(U)}{||U||} = 0$$
	
	\textit{Se define la \textbf{matriz diferencial} de $F$ en $C$ con incremento $U$ como la matriz $dF(C;U)$ de dimensiones $m\times p$ que verifique $\text{Vec}(dF(C;U))=A(C)\text{Vec}(U)$ y a la matriz $A(C)$ se le llama la \textbf{primera derivada} de $F$ en $C$.}
\end{definition}

Todas las propiedades de cálculo para las funciones matriciales se deducen de las correspondientes propiedades de las funciones vectoriales.

Tenemos los siguientes resultados análogos a los del caso vectorial:

\begin{theorem} (Primer teorema de identificación para funciones matriciales)
	Sea $F:S\subseteq \mathbb{M}_{n \times q}\rightarrow \mathbb{M}_{m \times p}$ diferencialbe en un punto interior $C$ de $S$. Entonces se verifica $\text{Vec}(dF(C;U))=A(C)\text{Vec}(U) \Leftrightarrow DF(C)=A(C)$.	
\end{theorem}

\begin{theorem} (Regla de la cadena para funcones matriciales)
	Sea $F:S\subseteq \mathbb{M}_{n \times q}\rightarrow \mathbb{M}_{m \times p}$ diferencialbe en un punto interior $C$ de $S$. Sea $T$ un subconjunto de $\mathbb{m\times p}$ tal que $F(X)\in T, \forall X\in S$ y supongamos que $G:T\rightarrow \mathbb{M}_{r\times s}$ es diferenciable en un punto $B$ ($B=F(C)$) de T. Entonces la función compuesta $H:S\rightarrow \mathbb{M}_{r\times s}$ definida por $H(X)=G(F(X))$ es diferenciable en $C$ y $DH(C)=DG(B)DF(C)$.
\end{theorem}

\begin{theorem}(Regla de invarianza de Cauchy para funciones matriciales)
	En las condiciones del teorema anterior, $dH(C;U)=dG(B;dF(C;U)), \; \forall U\in \mathbb{n\times q}$.	
\end{theorem}

\begin{corollary}
	Sean \textbf{$A_{txm}$, $B_{pxr}$ y $C_{sxu}$} matrices constantes y sean \textbf{$G_{mxp}$ y $H_{rxs}$} dos funciones matriciales diferenciables. Si \textbf{$F(X) = AG(X)BH(X)C$}, entonces
	\textbf{$$ dF(X)= AdG(X)BH(X)C + AG(X)BdH(X)C $$}
	y \textbf{$DF(X)= (C^tH^t(X)B^t \bigotimes A)DG(X) + (C^t \bigotimes AG(X)B)DH(X)$} 
\end{corollary}

\textcolor{red}{******************COPIAMOS LAS MILDOSCIENTAS34 PROPIEDADES????}

\begin{exercise}
	Sea $h: \mathbb{R}^k \rightarrow \mathbb{R}$ definida por $h(\beta) = (y-X\beta)^t(y-X\beta)$ donde $y \in \mathbb{R}^n$ y $X\in\mathbb{M}_{n\times k}.$ Haciendo uso de la regla de invarianza de Cauchy demostrar que
	$$dh(c;u) = dg(y-Xc;df(c;u)) = dg(y-Xc;-Xu)=-2(y-Xc)^tXu$$
	y con ello $Dh(c)=-2(y-Xc)^tX$.
\end{exercise}
\textit{Solución:}
	Para comenzar, tengamos en cuenta que h es composición de $f: \mathbb{R}^k \rightarrow \mathbb{R}^n$ y $g:\mathbb{R}^n \rightarrow \mathbb{R}$, con $f(c) = (y-Xc)$ con $X$ una matriz $nxk$ y con $y \in \mathbb{R}^n$; y $g(X) = X^tX$, de modo que $h(x)=g(f(x)) \forall x \in  \mathbb{R}^k$. Así, es claro que  $dh(c;u) = dg(y-Xc;df(c;u))$. ************************ Parece evidente pero no se que escribir xd****************
\begin{exercise}
	Sea $F(X)=AG(X)B$, donde $A_{m\times r}$ y $B_{s\times p}$ son matrices constantes y $G(X)_{r\times s}$ es una función diferenciable. Calcular $DF(C)$ a partir de la definición de diferencial matricial.
\end{exercise}
\textit{Solución:}

\begin{exercise}
	Si $X_{n\times n}$ es una matriz simétrica y $F: \mathbb{M}_{n\times q} \rightarrow \mathbb{M}_{m\times p}$ es diferenciable, demostrar que $d\text{Vec}(F(X)) = D_nDF(X)d\text{Vech}(X)$, mientras que $d\text{Vec}(F(X)) = N_nDF(X)d\text{Vec}(X)$ donde $N_n = \frac{1}{2}[I_{n^2}+K_{nn}]$.
\end{exercise}
\textit{Solución:}


\subsection{Matrices jacobianas y derivadas matriciales}

No existe una única definición para la derivada de una función de argumento matricial y esto supone un problema a la hora de usar el cálculo diferencial matricial.

Veamos las definiciones clásicas para funciones reales de argumento vectorial y vectoriales, tanto de argumento real como vectorial.

\begin{definition}
	\textit{Sea $f:\mathbb{R}^n\rightarrow \mathbb{R}$. Se define la derivada de $f$ respecto de $x\in \mathbb{R}^n$ como el vector $1\times n$ dado por $\displaystyle \frac{\partial f(x)}{\partial x^t} = \displaystyle \left( \frac{\partial f(x)}{\partial x_1},..., \frac{\partial f(x)}{\partial x_n} \right)$.}
\end{definition}

\begin{definition}
\textit{	Sea $f:\mathbb{R}\rightarrow \mathbb{R}^m$. Se define la derivada de $f$ respecto de $x\in \mathbb{R}$ como el vector $m\times 1$ dado por $\displaystyle \frac{\partial f(x)}{\partial x} = \displaystyle \left( \frac{\partial f_1(x)}{\partial x},..., \frac{\partial f_m(x)}{\partial x} \right)^t$.}
\end{definition}

\begin{definition}
\textit{	Sea $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$. Se define la derivada de $f$ respecto de $x\in \mathbb{R}^n$ como la matriz} $m\times n$
	$$ \frac{\partial f(x)}{\partial x^t} = \left( \begin{array}{c}
											\frac{\partial f_1(x)}{\partial x^t} \\ \vdots \\ \frac{\partial f_m(x)}{\partial x^t}
											\end{array}\right) =
											\left( \begin{array}{cccc}
											\frac{\partial f_1(x)}{\partial x_1} & \cdots & \cdots & \frac{\partial f_1(x)}{\partial x_n}\\
											\vdots & \vdots & \vdots & \vdots \\
											\frac{\partial f_m(x)}{\partial x_1} & \cdots & \cdots & \frac{\partial f_m(x)}{\partial x_n}
											\end{array}\right)$$
\end{definition}

\begin{definition}
\textit{	Sea $F:\mathbb{M}_{n\times q}\rightarrow \mathbb{R}$. Se define la derivada de $F$ respecto de $X\in \mathbb{M}_{n\times q}$ como la matriz $n\times q$}
	$$ \frac{\partial F(X)}{\partial X} = \left( \begin{array}{cccc}
			\frac{\partial F(X)}{\partial x_{11}} & \cdots & \cdots & \frac{\partial F(X)}{\partial x_{1q}}\\
			\vdots & \vdots & \vdots & \vdots \\
			\frac{\partial F(X)}{\partial x_{n1}} & \cdots & \cdots & \frac{\partial F(X)}{\partial x_{nq}}
	\end{array}\right)$$
\end{definition}

\begin{definition}
\textit{	Sea $F:\mathbb{R}\rightarrow \mathbb{M}_{m\times p}$. Se define la derivada de $F$ respecto de $x\in \mathbb{R}$ como la matriz $m\times p$}
	$$ \frac{\partial F(x)}{\partial x} = \left( \begin{array}{cccc}
			\frac{\partial F_{11}(x)}{\partial x} & \cdots & \cdots & \frac{\partial F_{1p}(x)}{\partial x}\\
			\vdots & \vdots & \vdots & \vdots \\
			\frac{\partial F_{m1}(x)}{\partial x} & \cdots & \cdots & \frac{\partial F_{mp}(x)}{\partial x}
	\end{array}\right)$$	
\end{definition}

\textcolor{red}{**********CREO que las importantes son solo las dos ultimas, quizas haya que borrar algunas de estas aunque eso se decidirá en un futuro xd ******}

Existen así muchas formas de definir la derivada de una función de argumento matricial, aunque ahora vamos a mostrar dos que generalizan a las demás:

\begin{definition}
	(Definición de derivada matricial según Mac Rae)\\
	\textit{Sea $F:\mathbb{M}_{n\times q}\rightarrow \mathbb{M}_{m\times p}$. Se define la derivada de $F$ respecto de $X\in \mathbb{M}_{n\times q}$ como la matriz $nm\times pq$}
	$$ \frac{\partial F(x)}{\partial x} = \left( \begin{array}{cccc}
	\frac{\partial F_{11}(x)}{\partial x} & \cdots & \cdots & \frac{\partial F_{1p}(x)}{\partial x}\\
	\vdots & \vdots & \vdots & \vdots \\
	\frac{\partial F_{m1}(x)}{\partial x} & \cdots & \cdots & \frac{\partial F_{mp}(x)}{\partial x}
	\end{array}\right)$$	
\end{definition}

\begin{definition}
	(Definición de derivada matricial según Dwyer)\\
\textit{	Sea $F:\mathbb{M}_{n\times q}\rightarrow \mathbb{M}_{m\times p}$. Se define la derivada de $F$ respecto de $X\in \mathbb{M}_{n\times q}$ como la matriz $nm\times pq$}
	$$ \frac{\partial F(x)}{\partial x} = \left( \begin{array}{cccc}
	\frac{\partial F(x)}{\partial x_{11}} & \cdots & \cdots & \frac{\partial F(x)}{\partial x_{1q}}\\
	\vdots & \vdots & \vdots & \vdots \\
	\frac{\partial F(x)}{\partial x_{n1}} & \cdots & \cdots & \frac{\partial F(x)}{\partial x_{nq}}
	\end{array}\right)$$	
\end{definition}

\begin{theorem}
		\textit{Sea $F:\mathbb{M}_{n\times q}\rightarrow \mathbb{M}_{m\times p}$. Entonces se verifica que $K_{nm}\frac{\partial F(x)}{\partial x}K_{pq}= \frac{\partial F(x)}{\partial x}$}
\end{theorem}

\begin{definition}
	(Definición de derivada matricial según Magnus y Neudecker)\\
\textit{	Sea $F:\mathbb{M}_{n\times q}\rightarrow \mathbb{M}_{m\times p}$. Se define la derivada de $F$ respecto de $X\in \mathbb{M}_{n\times q}$ como la matriz $mp\times nq$ dada por \textbf{$DF(X)=\frac{\partial Vec(F(x))}{\partial Vec^t(x)}$}}
\end{definition}

**********************************MILLONES Y MILLONES DE PROPIEDADES XDXDXDXDXDMUERO *************

\begin{theorem}
	\textit{Sea $F:\mathbb{M}_{n\times q}\rightarrow \mathbb{M}_{m\times p}$. Entonces }
	\begin{enumerate}
		\item $\frac{\partial F(x)}{\partial x^t} =(\frac{\partial (F(x))^t}{\partial x})^t $
		
		\item $\frac{\partial (F(x))^t}{\partial x^t} =(\frac{\partial F(x)}{\partial x})^t$
	\end{enumerate}
\end{theorem}

\begin{corollary}
\textit{	Sean \textbf{$A_{txm}$, $B_{pxr}$ y $C_{sxu}$} matrices constantes y sean \textbf{$G_{mxp}$ y $H_{rxs}$} dos funciones de argumento matricial $X_{n\times 	q}$. Si \textbf{$F(X) = AG(X)BH(X)C$}, entonces
	\textbf{$$ \frac{\partial F(\partial x)}{x}= (A \bigotimes I_n)\frac{\partial G(x)}{\partial x}(BH(\partial x)C \bigotimes I_q) + (AG(x)B \bigotimes I_n)\frac{\partial H(x)}{\partial x}(C \bigotimes I_q)$$}
	 }
\end{corollary}



\begin{exercise}
	A partir de las relaciones existentes entre la derivada matricial y la matriz jacobiana, verificar las siguientes expresiones:
	\begin{enumerate}[a)]
		\item Sea $X_{n\times n}$ y $F(X) = \text{tr}[X].$ Entonces $DF(X) = \text{Vec}^t(I_n).$
		\item Sea ahora $X_{n\times q}$ y $F(X) =X$. Entonces $DF(X) = I_q \otimes I_n = I_{nq}$.
	\end{enumerate}
\end{exercise}

\begin{exercise}
	Sea $X_{n\times q}$. Demostrar las siguientes igualdades:
	\begin{enumerate}[a)]
		\item $\displaystyle \frac{\partial X^t}{\partial X} = K_{qn}$.
		\item $\displaystyle \frac{\partial X}{\partial X^t} = K_{nq}$.
		\item $\displaystyle \frac{\partial X^t}{\partial X^t} = \text{Vec}(I_q)\text{Vec}^t(I_n)$.
	\end{enumerate}
\end{exercise}

\begin{exercise}
	Demostrar que si $X_{n\times n}$ es no singular entonces $\displaystyle \frac{\partial X^{-1}}{\partial X} = -\text{Vec}((X^{-1})^t)\text{Vec}^t(X^{-1})$.
\end{exercise}
\textit{Solución:}

\subsection{Derivadas matriciales de funciones escalares de un vector}

Las principales funciones que suelen aparecer en la práctica son:

\begin{itemize}
	\item Formas lineales: $\phi(x)=a^tx$
	\item Formas cuadráticas: $\phi(x)=x^tAx$
\end{itemize}

De las que se pueden sacar una serie de \textbf{propiedades:}

\begin{itemize}
	\item $d\phi(x) = a^tdx \rightarrow D\phi(x) = \frac{\partial \phi(x)}{\partial x^t} = a^t$
	\item $d\phi(x) = x^t(A+A^t)dx \rightarrow D\phi(x) = \frac{\partial \phi(x)}{\partial x^t} = x^t(A+A^t)$
	
\end{itemize}

Y otras tomando $f$ y $g$ como dos funciones vectoriales del vector x:

\begin{itemize}
	\item Si $\phi(x) = a^tf(x) \rightarrow D\phi(x) = a^tDf(x)$
	\item  Si $\phi(x) = f(x)^tg(x) \rightarrow D\phi(x) = g(x)^tDf(x)+ f(x)^tDg(x)$
	\item Si $\phi(x) = x^tAf(x) \rightarrow D\phi(x) = f(x)^t A^t + x^tADf(x)$
	\item Si $\phi(x) = f(x)^tAf(x) \rightarrow D\phi(x) = f(x)^t(A+A^t)Df(x)$
	\item Si $\phi(x) = f(x)^tAg(x) \rightarrow D\phi(x) = g(x)^tA^tDf(x)+ f(x)^tADg(x)$
	\item Si $x=(x_1^t,x_2^t)^t$ y Si $\phi(x) = x_1^tAx_2 \rightarrow D\phi(x) = x^t (\begin{array}{cc}
	0 & A\\
	A^t  & 0
	\end{array})$ 
\end{itemize}
	
\subsubsection{Derivadas matriciales de funciones escalares de matrices}


************PUEDE QUE FALTE LITERATURA EN MUCHAS PARTES XD*******

\subsubsection*{Derivadas matriciales asociadas a trazas}

\begin{theorem}
	Sea $F:\mathbb{M}_{n\times q}\rightarrow \mathbb{M}_{p\times p}$ y $\phi (x) = tr[]F(x)]$. Entonces $d\phi(x)= tr[dF(x)]$, $D\phi(x)= Vec^t(I_p)DF(x)$ y 
	$$ \frac{\partial \phi(x)}{\partial x} = I_p* \frac{\partial F(x)}{\partial x} = \sum_{l=1}^{p} \frac{\partial Fu(x)}{\partial x} $$ 
\end{theorem}

\begin{theorem}
	Sean $F_{p\times m}$, $G_{m\times p}$ dos funciones matriciales de $X_{n\times q}$. Entonces se verifica:
	\begin{itemize}
		\item $dtr[FG(x)] = tr[F(x)dG(x)]+ tr[F(x)dG(x)]$
		\item $Dtr[(FG)(x)] = Vec^t(G^t(x))DF(x) + Vec^t(F^t(x))DG(x)$
		\item $ \frac{\partial tr[(FG)(x)]}{\partial x} = G^t(x) * \frac{\partial F(x)}{\partial x} + F^t(x) * \frac{\partial G(x)}{\partial x}$
	\end{itemize}
\end{theorem}



\subsection{Diferencial segunda y hessianos}


	
\end{document}